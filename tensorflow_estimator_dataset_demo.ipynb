{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.13.1\n"
     ]
    }
   ],
   "source": [
    "# This is the complete code for the following blogpost:\n",
    "# https://developers.googleblog.com/2017/09/introducing-tensorflow-datasets.html\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "import six.moves.urllib.request as request\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check that we have correct TensorFlow version installed\n",
    "tf_version = tf.__version__\n",
    "print(\"TensorFlow version: {}\".format(tf_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows users: You only need to change PATH, rest is platform independent\n",
    "PATH = \"./dataset\"\n",
    "CHECK_POINTS_PATH = \"./checkpoints\"\n",
    "\n",
    "# Fetch and store Training and Test dataset files\n",
    "PATH_DATASET = PATH + os.sep + \"iris\"\n",
    "FILE_TRAIN = PATH_DATASET + os.sep + \"iris_training.csv\"\n",
    "FILE_TEST = PATH_DATASET + os.sep + \"iris_test.csv\"\n",
    "URL_TRAIN = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "URL_TEST = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "\n",
    "def downloadDataset(url, file):\n",
    "    if not os.path.exists(PATH_DATASET):\n",
    "        os.makedirs(PATH_DATASET)\n",
    "    if not os.path.exists(file):\n",
    "        data = request.urlopen(url).read()\n",
    "        with open(file, \"wb\") as f:\n",
    "            f.write(data)\n",
    "            f.close()\n",
    "downloadDataset(URL_TRAIN, FILE_TRAIN)\n",
    "downloadDataset(URL_TEST, FILE_TEST)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The CSV features in our training & test data\n",
    "feature_names = ['SepalLength','SepalWidth','PetalLength','PetalWidth']\n",
    "\n",
    "# Create an input function reading a file using the Dataset API\n",
    "# Then provide the results to the Estimator API\n",
    "\n",
    "def my_input_fn(file_path, perform_shuffle=False, repeat_count=1):\n",
    "    def decode_csv(line):\n",
    "        parsed_line = tf.decode_csv(line, [[0.], [0.], [0.], [0.], [0]])\n",
    "        label = parsed_line[-1]  # Last element is the label\n",
    "        del parsed_line[-1]  # Delete last element\n",
    "        features = parsed_line  # Everything but last elements are the features\n",
    "        d = dict(zip(feature_names, features)), label\n",
    "        return d\n",
    "\n",
    "    dataset = (tf.data.TextLineDataset(file_path)  # Read text file\n",
    "               .skip(1)  # Skip header row\n",
    "               .map(decode_csv))  # Transform each elem by applying decode_csv fn\n",
    "    if perform_shuffle:\n",
    "        # Randomizes input using a window of 256 elements (read into memory)\n",
    "        dataset = dataset.shuffle(buffer_size=256)\n",
    "    dataset = dataset.repeat(repeat_count)  # Repeats dataset this # times\n",
    "    dataset = dataset.batch(32)  # Batch size to use\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    batch_features, batch_labels = iterator.get_next()\n",
    "    return batch_features, batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PetalLength': <tf.Tensor 'IteratorGetNext:0' shape=(?,) dtype=float32>,\n",
       "  'PetalWidth': <tf.Tensor 'IteratorGetNext:1' shape=(?,) dtype=float32>,\n",
       "  'SepalLength': <tf.Tensor 'IteratorGetNext:2' shape=(?,) dtype=float32>,\n",
       "  'SepalWidth': <tf.Tensor 'IteratorGetNext:3' shape=(?,) dtype=float32>},\n",
       " <tf.Tensor 'IteratorGetNext:4' shape=(?,) dtype=int32>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_batch = my_input_fn(FILE_TRAIN, True)  # Will return 32 random elements\n",
    "next_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the feature_columns, which specifies the input to our model\n",
    "# All our input features are numeric, so use numeric_column for each one\n",
    "feature_columns = [tf.feature_column.numeric_column(k) for k in feature_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "# from tensorflow_estimator.python.estimator import run_config as run_config_lib\n",
    "# # 配置GPU参数\n",
    "# session_config = tf.compat.v1.ConfigProto()\n",
    "# session_config.gpu_options.allocator_type = 'BFC' #A \"Best-fit with coalescing\" algorithm, simplified from a version of dlmalloc.\n",
    "# session_config.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "# session_config.gpu_options.allow_growth = True\n",
    "# config = run_config_lib.RunConfig().replace(session_config=session_config)\n",
    "\n",
    "# Create a deep neural network regression classifier\n",
    "# Use the DNNClassifier pre-made estimator\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=feature_columns,  # The input features to our model\n",
    "    hidden_units=[10, 10],  # Two layers, each with 10 neurons\n",
    "    n_classes=3,\n",
    "    model_dir=CHECK_POINTS_PATH,\n",
    "    config=config\n",
    "    )  # Path to where checkpoints etc are stored\n",
    "\n",
    "# Train our model, use the previously function my_input_fn\n",
    "# Input to training is a file with training example\n",
    "# Stop training after 8 iterations of train data (epochs)\n",
    "classifier.train(\n",
    "    input_fn=lambda: my_input_fn(FILE_TRAIN, True, 8))\n",
    "\n",
    "# Evaluate our model using the examples contained in FILE_TEST\n",
    "# Return value will contain evaluation_metrics such as: loss & average_loss\n",
    "evaluate_result = classifier.evaluate(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 4))\n",
    "    \n",
    "print(\"Evaluation results\")\n",
    "for key in evaluate_result:\n",
    "    print(\"   {}, was: {}\".format(key, evaluate_result[key]))\n",
    "\n",
    "# Predict the type of some Iris flowers.\n",
    "# Let's predict the examples in FILE_TEST, repeat only once.\n",
    "predict_results = classifier.predict(\n",
    "    input_fn=lambda: my_input_fn(FILE_TEST, False, 1))\n",
    "print(\"Predictions on test file\")\n",
    "for prediction in predict_results:\n",
    "    # Will print the predicted class, i.e: 0, 1, or 2 if the prediction\n",
    "    # is Iris Sentosa, Vericolor, Virginica, respectively.\n",
    "    print(prediction[\"class_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ./checkpoints/model.ckpt-30\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "  I think: [5.9, 3.0, 4.2, 1.5], is Iris Versicolor\n",
      "  I think: [6.9, 3.1, 5.4, 2.1], is Iris Virginica\n",
      "  I think: [5.1, 3.3, 1.7, 0.5], is Iris Sentosa\n"
     ]
    }
   ],
   "source": [
    "# Let create a dataset for prediction\n",
    "# We've taken the first 3 examples in FILE_TEST\n",
    "prediction_input = [[5.9, 3.0, 4.2, 1.5],  # -> 1, Iris Versicolor\n",
    "                    [6.9, 3.1, 5.4, 2.1],  # -> 2, Iris Virginica\n",
    "                    [5.1, 3.3, 1.7, 0.5]]  # -> 0, Iris Sentosa\n",
    "\n",
    "def new_input_fn():\n",
    "    def decode(x):\n",
    "        x = tf.split(x, 4)  # Need to split into our 4 features\n",
    "        return dict(zip(feature_names, x))  # To build a dict of them\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(prediction_input)\n",
    "    dataset = dataset.map(decode)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    next_feature_batch = iterator.get_next()\n",
    "    return next_feature_batch, None  # In prediction, we have no labels\n",
    "\n",
    "# Predict all our prediction_input\n",
    "predict_results = classifier.predict(input_fn=new_input_fn)\n",
    "\n",
    "# Print results\n",
    "print(\"Predictions:\")\n",
    "for idx, prediction in enumerate(predict_results):\n",
    "    type = prediction[\"class_ids\"][0]  # Get the predicted class (index)\n",
    "    if type == 0:\n",
    "        print(\"  I think: {}, is Iris Sentosa\".format(prediction_input[idx]))\n",
    "    elif type == 1:\n",
    "        print(\"  I think: {}, is Iris Versicolor\".format(prediction_input[idx]))\n",
    "    else:\n",
    "        print(\"  I think: {}, is Iris Virginica\".format(prediction_input[idx]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 ('tf13_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dcfc5ac383e29a4d9e4b55ea7ba22cf581c6948bbd5183707ae4e9a2e91d8d92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
