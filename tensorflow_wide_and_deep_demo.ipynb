{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据\n",
    "\n",
    "使用criteo数据集，包括train.csv,test.csv等两个文件。具体数据字段主要分为以下三类：\n",
    "\n",
    "Label - Target variable that indicates if an ad was clicked (1) or not (0).（待预测广告，被点击是1，没有被点击是0。）\n",
    "\n",
    "I1-I13 - A total of 13 columns of integer features (mostly count features).（总共 13 列数值型特征，主要是计数特征。）\n",
    "\n",
    "C1-C26 - A total of 26 columns of categorical features. The values of these features have been hashed onto 32 bits for anonymization purposes. （共有 26 列类别型特征。 出于匿名目的，特征值已散列到 32 位。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/yqstar/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1\n",
      "0.9.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from deepctr.models import DeepFM,WDL\n",
    "from deepctr.feature_column import SparseFeat, DenseFeat, get_feature_names\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import deepctr\n",
    "\n",
    "print(tf.__version__)\n",
    "print(deepctr.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://arxiv.org/pdf/1606.07792.pdf\n",
    "\n",
    "# -*- coding:utf-8 -*-\n",
    "\"\"\"\n",
    "Author:\n",
    "    Weichen Shen, weichenswc@163.com\n",
    "Reference:\n",
    "    [1] Cheng H T, Koc L, Harmsen J, et al. Wide & deep learning for recommender systems[C]//Proceedings of the 1st Workshop on Deep Learning for Recommender Systems. ACM, 2016: 7-10.(https://arxiv.org/pdf/1606.07792.pdf)\n",
    "\"\"\"\n",
    "\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "\n",
    "from deepctr.feature_column import build_input_features, get_linear_logit, input_from_feature_columns\n",
    "from deepctr.layers.core import PredictionLayer, DNN\n",
    "from deepctr.layers.utils import add_func, combined_dnn_input\n",
    "\n",
    "\n",
    "def WDL(linear_feature_columns, dnn_feature_columns, dnn_hidden_units=(256, 128, 64), l2_reg_linear=0.00001,\n",
    "        l2_reg_embedding=0.00001, l2_reg_dnn=0, seed=1024, dnn_dropout=0, dnn_activation='relu',\n",
    "        task='binary'):\n",
    "    \"\"\"Instantiates the Wide&Deep Learning architecture.\n",
    "    :param linear_feature_columns: An iterable containing all the features used by linear part of the model.\n",
    "    :param dnn_feature_columns: An iterable containing all the features used by deep part of the model.\n",
    "    :param dnn_hidden_units: list,list of positive integer or empty list, the layer number and units in each layer of DNN\n",
    "    :param l2_reg_linear: float. L2 regularizer strength applied to wide part\n",
    "    :param l2_reg_embedding: float. L2 regularizer strength applied to embedding vector\n",
    "    :param l2_reg_dnn: float. L2 regularizer strength applied to DNN\n",
    "    :param seed: integer ,to use as random seed.\n",
    "    :param dnn_dropout: float in [0,1), the probability we will drop out a given DNN coordinate.\n",
    "    :param dnn_activation: Activation function to use in DNN\n",
    "    :param task: str, ``\"binary\"`` for  binary logloss or  ``\"regression\"`` for regression loss\n",
    "    :return: A Keras model instance.\n",
    "    \"\"\"\n",
    "\n",
    "    features = build_input_features(\n",
    "        linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    inputs_list = list(features.values())\n",
    "\n",
    "    linear_logit = get_linear_logit(features, linear_feature_columns, seed=seed, prefix='linear',\n",
    "                                    l2_reg=l2_reg_linear)\n",
    "\n",
    "    sparse_embedding_list, dense_value_list = input_from_feature_columns(features, dnn_feature_columns,\n",
    "                                                                         l2_reg_embedding, seed)\n",
    "\n",
    "    dnn_input = combined_dnn_input(sparse_embedding_list, dense_value_list)\n",
    "    dnn_out = DNN(dnn_hidden_units, dnn_activation, l2_reg_dnn, dnn_dropout, False, seed=seed)(dnn_input)\n",
    "    dnn_logit = Dense(1, use_bias=False)(dnn_out)\n",
    "\n",
    "    final_logit = add_func([dnn_logit, linear_logit])\n",
    "\n",
    "    output = PredictionLayer(task)(final_logit)\n",
    "\n",
    "    model = Model(inputs=inputs_list, outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-94fdbc9da225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/home/yqstar/ai_project/CTR_Algorithm/DeepCTR/examples/criteo_sample.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msparse_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'C'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m27\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdense_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'I'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m14\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    data = pd.read_csv('/home/yqstar/ai_project/CTR_Algorithm/DeepCTR/examples/criteo_sample.txt')\n",
    "\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    fixlen_feature_columns = [SparseFeat(feat, vocabulary_size=data[feat].max() + 1, embedding_dim=4) for i, feat in enumerate(sparse_features)] + [DenseFeat(feat, 1, ) for feat in dense_features]\n",
    "\n",
    "    dnn_feature_columns = fixlen_feature_columns\n",
    "    linear_feature_columns = fixlen_feature_columns\n",
    "\n",
    "    feature_names = get_feature_names(linear_feature_columns + dnn_feature_columns)\n",
    "\n",
    "    # 3.generate input data for model\n",
    "\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=2020)\n",
    "    train_model_input = {name: train[name] for name in feature_names}\n",
    "    test_model_input = {name: test[name] for name in feature_names}\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    model = WDL(linear_feature_columns, dnn_feature_columns, task='binary')\n",
    "    model.compile(\"adam\", \"binary_crossentropy\",metrics=['binary_crossentropy'])\n",
    "\n",
    "    history = model.fit(train_model_input, \n",
    "                        train[target].values,\n",
    "                        batch_size=256, \n",
    "                        epochs=1, \n",
    "                        verbose=2, \n",
    "                        validation_split=0.2)\n",
    "    \n",
    "    pred_ans = model.predict(test_model_input, batch_size=256)\n",
    "    print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpxfsjdlk4\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpxfsjdlk4', '_tf_random_seed': 2021, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f02ac277630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpxfsjdlk4/model.ckpt.\n",
      "INFO:tensorflow:loss = 492.37613, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpxfsjdlk4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 492.37613.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "make sure the activation function use training flag properly call() got an unexpected keyword argument 'training'\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpxfsjdlk4/model.ckpt-1\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "from deepctr.estimator import DeepFMEstimator\n",
    "from deepctr.estimator.inputs import input_fn_pandas\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取Criteo数据集\n",
    "    data = pd.read_csv('./dataset/criteo_sample.txt')\n",
    "\n",
    "    # Criteo数据集的sparse特征和dense特征字段名\n",
    "    sparse_features = ['C' + str(i) for i in range(1, 27)]\n",
    "    dense_features = ['I' + str(i) for i in range(1, 14)]\n",
    "\n",
    "    # 缺失值填充：sparse_features使用-1填充；dense_features使用0填充\n",
    "    data[sparse_features] = data[sparse_features].fillna('-1', )\n",
    "    data[dense_features] = data[dense_features].fillna(0, )\n",
    "\n",
    "    # label数据赋值target\n",
    "    target = ['label']\n",
    "\n",
    "    # 1.Label Encoding for sparse features,and do simple Transformation for dense features\n",
    "    # sparse特征采用LabelEncoder；dense特征采用最大最小化缩放。\n",
    "    for feat in sparse_features:\n",
    "        lbe = LabelEncoder()\n",
    "        data[feat] = lbe.fit_transform(data[feat])\n",
    "    mms = MinMaxScaler(feature_range=(0, 1))\n",
    "    data[dense_features] = mms.fit_transform(data[dense_features])\n",
    "\n",
    "    # 2.count #unique features for each sparse field,and record dense feature field name\n",
    "\n",
    "    dnn_feature_columns = []\n",
    "    linear_feature_columns = []\n",
    "\n",
    "    for i, feat in enumerate(sparse_features):\n",
    "        # tf.feature_column.categorical_column_with_identity(key, num_buckets, default_value=None):将数据转为OneHot数据\n",
    "        # tf.feature_column.embedding_column()：OneHot数据转为embedding数据\n",
    "        # tf.feature_column.numeric_column():\n",
    "        dnn_feature_columns.append(tf.feature_column.embedding_column(\n",
    "            tf.feature_column.categorical_column_with_identity(feat, data[feat].max() + 1), 4))\n",
    "        linear_feature_columns.append(tf.feature_column.categorical_column_with_identity(feat, data[feat].max() + 1))\n",
    "    for feat in dense_features:\n",
    "        dnn_feature_columns.append(tf.feature_column.numeric_column(feat))\n",
    "        linear_feature_columns.append(tf.feature_column.numeric_column(feat))\n",
    "\n",
    "    # 3.generate input data for model\n",
    "    train, test = train_test_split(data, test_size=0.2, random_state=2021)\n",
    "\n",
    "    # Not setting default value for continuous feature. filled with mean.\n",
    "    # 构建input_fn函数，本示例使用input_fn_pandas函数\n",
    "    train_model_input = input_fn_pandas(train, sparse_features + dense_features, 'label', shuffle=True)\n",
    "    test_model_input = input_fn_pandas(test, sparse_features + dense_features, None, shuffle=False)\n",
    "\n",
    "    # 4.Define Model,train,predict and evaluate\n",
    "    model = DeepFMEstimator(linear_feature_columns, dnn_feature_columns, task='binary',\n",
    "                            config=tf.estimator.RunConfig(tf_random_seed=2021))\n",
    "\n",
    "    model.train(train_model_input)\n",
    "    pred_ans_iter = model.predict(test_model_input)\n",
    "    # pred_ans = list(map(lambda x: x['pred'], pred_ans_iter))\n",
    "    predictions = list(itertools.islice(pred_ans_iter,1))\n",
    "\n",
    "    \n",
    "    #\n",
    "    # print(\"test LogLoss\", round(log_loss(test[target].values, pred_ans), 4))\n",
    "    # print(\"test AUC\", round(roc_auc_score(test[target].values, pred_ans), 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'logits': array([-3.4694154], dtype=float32),\n",
       "  'pred': array([0.03019512], dtype=float32)}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_fn定义\n",
    "## column定义\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"./dataset/criteo_sample.txt\")\n",
    "\n",
    "sparse_feat_list = [\"C\" + str(i) for i in range(1,27)]\n",
    "dense_feat_list =  [\"I\" + str(i) for i in range(1,13)]\n",
    "\n",
    "data[sparse_feat_list] = data[sparse_feat_list].fillna(-1)\n",
    "data[dense_feat_list] = data[dense_feat_list].fillna(0)\n",
    "\n",
    "# 特征列\n",
    "for i,feat in enumerate(sparse_feat_list):\n",
    "    tf.feature_column.categorical_column_with_identity(sparse_feat_list[i], 100)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>I1</th>\n",
       "      <th>I2</th>\n",
       "      <th>I3</th>\n",
       "      <th>I4</th>\n",
       "      <th>I5</th>\n",
       "      <th>I6</th>\n",
       "      <th>I7</th>\n",
       "      <th>I8</th>\n",
       "      <th>I9</th>\n",
       "      <th>...</th>\n",
       "      <th>C17</th>\n",
       "      <th>C18</th>\n",
       "      <th>C19</th>\n",
       "      <th>C20</th>\n",
       "      <th>C21</th>\n",
       "      <th>C22</th>\n",
       "      <th>C23</th>\n",
       "      <th>C24</th>\n",
       "      <th>C25</th>\n",
       "      <th>C26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>260.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17668.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>87c6f83c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>0429f84b</td>\n",
       "      <td>-1</td>\n",
       "      <td>3a171ecb</td>\n",
       "      <td>c0d61a5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>30251.0</td>\n",
       "      <td>247.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>...</td>\n",
       "      <td>d4bb7bd8</td>\n",
       "      <td>6fc84bfb</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>5155d8a3</td>\n",
       "      <td>-1</td>\n",
       "      <td>be7c41b4</td>\n",
       "      <td>ded4aac9</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>164.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>523.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>675c9258</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>2e01979f</td>\n",
       "      <td>-1</td>\n",
       "      <td>bcdee96c</td>\n",
       "      <td>6d5d1302</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>16836.0</td>\n",
       "      <td>200.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>52e44668</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>e587c466</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>3b183c5c</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>...</td>\n",
       "      <td>e5ba7672</td>\n",
       "      <td>25c88e42</td>\n",
       "      <td>21ddcdc9</td>\n",
       "      <td>b1252a9d</td>\n",
       "      <td>0e8585d2</td>\n",
       "      <td>-1</td>\n",
       "      <td>32c7478e</td>\n",
       "      <td>0d4a6d1a</td>\n",
       "      <td>001f3601</td>\n",
       "      <td>92c878de</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label   I1  I2     I3    I4       I5     I6   I7    I8     I9  ...  \\\n",
       "0      0  0.0   3  260.0   0.0  17668.0    0.0  0.0  33.0    0.0  ...   \n",
       "1      0  0.0  -1   19.0  35.0  30251.0  247.0  1.0  35.0  160.0  ...   \n",
       "2      0  0.0   0    2.0  12.0   2013.0  164.0  6.0  35.0  523.0  ...   \n",
       "3      0  0.0  13    1.0   4.0  16836.0  200.0  5.0   4.0   29.0  ...   \n",
       "4      0  0.0   0  104.0  27.0   1990.0  142.0  4.0  32.0   37.0  ...   \n",
       "\n",
       "        C17       C18       C19       C20       C21 C22       C23       C24  \\\n",
       "0  e5ba7672  87c6f83c        -1        -1  0429f84b  -1  3a171ecb  c0d61a5c   \n",
       "1  d4bb7bd8  6fc84bfb        -1        -1  5155d8a3  -1  be7c41b4  ded4aac9   \n",
       "2  e5ba7672  675c9258        -1        -1  2e01979f  -1  bcdee96c  6d5d1302   \n",
       "3  e5ba7672  52e44668        -1        -1  e587c466  -1  32c7478e  3b183c5c   \n",
       "4  e5ba7672  25c88e42  21ddcdc9  b1252a9d  0e8585d2  -1  32c7478e  0d4a6d1a   \n",
       "\n",
       "        C25       C26  \n",
       "0        -1        -1  \n",
       "1        -1        -1  \n",
       "2        -1        -1  \n",
       "3        -1        -1  \n",
       "4  001f3601  92c878de  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特征列使用\n",
    "\n",
    "【TF】tf.feature_column特征处理方法汇总：https://blog.csdn.net/pearl8899/article/details/107936792\n",
    "\n",
    "使用tf.feature_column函数创建特征列返回：CategoricalColumn 对象或者 DenseColumn 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "———————1———————\n",
      "{'birthplace': [[0], [1], [1], [3], [4], [5], [6], [11]]}\n",
      "———————2———————\n",
      "IdentityCategoricalColumn(key='birthplace', number_buckets=10, default_value=0)\n",
      "——————3————————\n",
      "IndicatorColumn(categorical_column=IdentityCategoricalColumn(key='birthplace', number_buckets=10, default_value=0))\n",
      "———————4———————\n",
      "Tensor(\"input_layer_1/concat:0\", shape=(8, 10), dtype=float32)\n",
      "———————5———————\n",
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    " \n",
    "if __name__:\n",
    "    # 特征数据\n",
    "    features = {\n",
    "        'birthplace': [[0], [1], [1], [3], [4], [5], [6], [11]]\n",
    "    }\n",
    "    print (\"———————1———————\")\n",
    "    print (features)\n",
    "    # 特征列\n",
    "    birthplace = tf.feature_column.categorical_column_with_identity(\"birthplace\", num_buckets=10, default_value=0)\n",
    "    print (\"———————2———————\")\n",
    "    print (birthplace)\n",
    "    birthplace = tf.feature_column.indicator_column(birthplace)\n",
    "    print (\"——————3————————\")\n",
    "    print (birthplace)\n",
    "    # 组合特征列\n",
    "    columns = [birthplace]\n",
    "    # 输入层（数据，特征列）\n",
    "    inputs = tf.feature_column.input_layer(features, columns)\n",
    "    print (\"———————4———————\"  )  \n",
    "    print (inputs)\n",
    "    # 初始化并运行\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(tf.tables_initializer())\n",
    "    sess.run(init)\n",
    "    v = sess.run(inputs)\n",
    "    print (\"———————5———————\")\n",
    "    print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "categorical_column_with_identity() got an unexpected keyword argument 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-75f670d71509>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#特征列\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdepartment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_column_with_identity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'department'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"—————1—————\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# print columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: categorical_column_with_identity() got an unexpected keyword argument 'dtype'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "sess=tf.Session()\n",
    "#特征数据\n",
    "features = {\n",
    "    'department': ['sport', 'sport', 'drawing', 'gardening', 'travelling'],\n",
    "}\n",
    "#特征列\n",
    "department = tf.feature_column.categorical_column_with_hash_bucket('department', 10, dtype=tf.string)\n",
    "print (\"—————1—————\")\n",
    "# print columns\n",
    "columns = tf.feature_column.embedding_column(department, dimension=20)\n",
    "print(columns)\n",
    "#输入层（数据，特征列）\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "#初始化并运行\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(init)\n",
    " \n",
    "v=sess.run(inputs)\n",
    "# p=sess.run(department)\n",
    "print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Items of feature_columns must be a _DenseColumn. You can wrap a categorical column with an embedding_column or indicator_column. Given: HashedCategoricalColumn(key='item_id', hash_bucket_size=4, dtype=tf.int8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e5e3d62a4e0e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                              dtype=tf.int8)\n\u001b[1;32m      8\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mitem_id_hash\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;31m#初始化并运行\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0minit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36minput_layer\u001b[0;34m(features, feature_columns, weight_collections, trainable, cols_to_vars, cols_to_output_tensors)\u001b[0m\n\u001b[1;32m    300\u001b[0m       \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m       \u001b[0mcols_to_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols_to_vars\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m       cols_to_output_tensors=cols_to_output_tensors)\n\u001b[0m\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/rs_ctr/lib/python3.6/site-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m_internal_input_layer\u001b[0;34m(features, feature_columns, weight_collections, trainable, cols_to_vars, scope, cols_to_output_tensors, from_template)\u001b[0m\n\u001b[1;32m    185\u001b[0m           \u001b[0;34m'Items of feature_columns must be a _DenseColumn. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m           \u001b[0;34m'You can wrap a categorical column with an '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m           'embedding_column or indicator_column. Given: {}'.format(column))\n\u001b[0m\u001b[1;32m    188\u001b[0m   \u001b[0mweight_collections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight_collections\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGLOBAL_VARIABLES\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mweight_collections\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Items of feature_columns must be a _DenseColumn. You can wrap a categorical column with an embedding_column or indicator_column. Given: HashedCategoricalColumn(key='item_id', hash_bucket_size=4, dtype=tf.int8)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "features = {'item_id': [1, 2]}\n",
    "item_id = tf.feature_column.indicator_column(\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('item_id', hash_bucket_size=4,\n",
    "                                                             dtype=tf.int8))\n",
    "item_id_hash=tf.feature_column.categorical_column_with_hash_bucket('item_id', hash_bucket_size=4,\n",
    "                                                             dtype=tf.int8)\n",
    "columns = [item_id_hash]\n",
    "inputs = tf.feature_column.input_layer(features, columns)\n",
    "#初始化并运行\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(tf.tables_initializer())\n",
    "sess.run(init)\n",
    " \n",
    "v=sess.run(inputs)\n",
    "# p=sess.run(department)\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import itertools\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "\n",
    "def input_fn_pandas(df, features, label=None, batch_size=256, num_epochs=5, shuffle=False, queue_capacity_factor=10,\n",
    "                    num_threads=1):\n",
    "    if label is not None:\n",
    "        y = df[label]\n",
    "    else:\n",
    "        y = None\n",
    "    if tf.__version__ >= \"2.0.0\":\n",
    "        return tf.compat.v1.estimator.inputs.pandas_input_fn(df[features], y, batch_size=batch_size,\n",
    "                                                             num_epochs=num_epochs,\n",
    "                                                             shuffle=shuffle,\n",
    "                                                             queue_capacity=batch_size * queue_capacity_factor,\n",
    "                                                             num_threads=num_threads)\n",
    "\n",
    "    return tf.estimator.inputs.pandas_input_fn(df[features], y, batch_size=batch_size, num_epochs=num_epochs,\n",
    "                                               shuffle=shuffle, queue_capacity=batch_size * queue_capacity_factor,\n",
    "                                               num_threads=num_threads)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('rs_ctr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "91161dda26a099182a442a37648587c784d389ce8425a0dcb5cdbd786816510f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
